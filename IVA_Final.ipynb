{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e13b08a8-ae72-4250-bce8-b195e36dd0b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "58a775be-ee24-4a8b-a463-169393499551",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\kmano\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Loading checkpoint shards: 100%|█████████████████████████████████████████████████████████| 3/3 [00:37<00:00, 12.52s/it]\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on local URL:  http://127.0.0.1:7860\n",
      "Running on public URL: https://ecd3e9415d4e09c72c.gradio.live\n",
      "\n",
      "This share link expires in 72 hours. For free permanent hosting and GPU upgrades, run `gradio deploy` from Terminal to deploy to Spaces (https://huggingface.co/spaces)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"https://ecd3e9415d4e09c72c.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Keyboard interruption in main thread... closing server.\n",
      "Killing tunnel 127.0.0.1:7860 <> https://ecd3e9415d4e09c72c.gradio.live\n"
     ]
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Launching without log\n",
    "import torch\n",
    "from transformers import BitsAndBytesConfig, pipeline\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "from PIL import Image\n",
    "import re\n",
    "import whisper\n",
    "import gradio as gr\n",
    "import os\n",
    "from gtts import gTTS\n",
    "\n",
    "quantization_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_compute_dtype=torch.float16\n",
    ")\n",
    "\n",
    "model_id = \"llava-hf/llava-1.5-7b-hf\"\n",
    "pipe = pipeline(\"image-to-text\", model=model_id, \n",
    "                model_kwargs={\"quantization_config\": quantization_config})\n",
    "\n",
    "# CUDA device check\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "#print(f\"Using torch {torch._version_} ({DEVICE})\")\n",
    "\n",
    "whisper_model = whisper.load_model(\"base\", device=DEVICE)\n",
    "\n",
    "def transcribe(audio_path):\n",
    "    if not audio_path:\n",
    "        return '', '', None\n",
    "\n",
    "    audio = whisper.load_audio(audio_path)\n",
    "    audio = whisper.pad_or_trim(audio)\n",
    "\n",
    "    mel = whisper.log_mel_spectrogram(audio).to(whisper_model.device)\n",
    "\n",
    "    options = whisper.DecodingOptions(language=\"en\")\n",
    "    result = whisper.decode(whisper_model, mel, options)\n",
    "    return result.text\n",
    "\n",
    "def text_to_speech(text, file_path):\n",
    "    language = 'en'\n",
    "    audio_obj = gTTS(text=text, lang=language, slow=False)\n",
    "    audio_obj.save(file_path)\n",
    "    return file_path\n",
    "\n",
    "def img2txt(input_text, input_image):\n",
    "    image = Image.open(input_image)\n",
    "    prompt_instructions = f\"Act as an expert in imagery descriptive analysis, respond to the prompt: {input_text}\"\n",
    "    prompt = f\"USER: <image>\\n{prompt_instructions}\\nASSISTANT:\"\n",
    "    \n",
    "    outputs = pipe(image, prompt=prompt, generate_kwargs={\"max_new_tokens\": 6000})\n",
    "    \n",
    "    if outputs and len(outputs[0][\"generated_text\"]) > 0:\n",
    "        match = re.search(r'ASSISTANT:\\s*(.*)', outputs[0][\"generated_text\"])\n",
    "        if match:\n",
    "            reply = match.group(1)\n",
    "        else:\n",
    "            reply = \"No response found.\"\n",
    "    else:\n",
    "        reply = \"No response generated.\"\n",
    "    return reply\n",
    "\n",
    "def process_inputs(text_input, audio_path, image_path):\n",
    "    speech_to_text_output = ''\n",
    "    if audio_path:\n",
    "        speech_to_text_output = transcribe(audio_path)\n",
    "    \n",
    "    if text_input:\n",
    "        combined_input = text_input\n",
    "    else:\n",
    "        combined_input = speech_to_text_output\n",
    "\n",
    "    if not combined_input:\n",
    "        combined_input = \"Describe this image.\"\n",
    "\n",
    "    if image_path:\n",
    "        chatgpt_output = img2txt(combined_input, image_path)\n",
    "    else:\n",
    "        image_path=\"C:/Users/kmano/OneDrive/Documents/Certificates/blk.png\"  # Default image if none provided\n",
    "        chatgpt_output = img2txt(combined_input, image_path)\n",
    "    \n",
    "    processed_audio_path = text_to_speech(chatgpt_output, \"Temp3.mp3\") if chatgpt_output else None\n",
    "\n",
    "    return speech_to_text_output, chatgpt_output, processed_audio_path\n",
    "\n",
    "iface = gr.Interface(\n",
    "    fn=process_inputs,\n",
    "    inputs=[\n",
    "        gr.Textbox(label=\"Text Input\"),\n",
    "        gr.Audio(type=\"filepath\", label=\"Audio Input\"),  \n",
    "        gr.Image(type=\"filepath\", label=\"Image Input\")   \n",
    "               \n",
    "    ],\n",
    "    outputs=[\n",
    "        gr.Textbox(label=\"Speech to Text Output\"),  \n",
    "        gr.Textbox(label=\"Generated Response\"),     \n",
    "        gr.Audio(label=\"Text to Speech Output\")    \n",
    "    ],\n",
    "    title=\"Intelligent Voice and Text Assistant using Llava\",\n",
    "    description=\"Upload an image, provide voice input, or type a prompt to receive a detailed analysis and audio response.\"\n",
    ")\n",
    "\n",
    "iface.launch(debug=True,share=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f5050461-1d89-4d0c-831c-72e3596a0167",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9c47ff46-0085-45b2-9011-c8aa92175a8c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\kmano\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Loading checkpoint shards: 100%|█████████████████████████████████████████████████████████| 3/3 [00:28<00:00,  9.49s/it]\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on local URL:  http://127.0.0.1:7860\n",
      "Running on public URL: https://f4d89f1eebcbb251bd.gradio.live\n",
      "\n",
      "This share link expires in 72 hours. For free permanent hosting and GPU upgrades, run `gradio deploy` from Terminal to deploy to Spaces (https://huggingface.co/spaces)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"https://f4d89f1eebcbb251bd.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "*******NEW DATA*******\n",
      "User:192.168.1.7\n",
      "Text Input:What is CRDI Engine\n",
      "Audio input:\n",
      "Output:CRDI stands for Compact and Rural Diesel Initiative. It is a technology developed by the Indian government to improve the efficiency and performance of diesel engines in rural and remote areas. The CRDI engine is designed to operate on low-quality diesel fuel, which is commonly available in rural areas. The engine is equipped with advanced features such as improved combustion, better fuel atomization, and enhanced engine control systems. These features help to optimize the engine's performance, reduce emissions, and improve overall engine efficiency, making it an ideal choice for rural and remote locations.\n",
      "\n",
      "*******NEW DATA*******\n",
      "User:192.168.1.7\n",
      "Text Input:What is CRDI \n",
      "Audio input:\n",
      "Output:CRDI stands for \"Cognitive Reasoning and Decision Instrument.\" It is a psychometric tool used to assess an individual's cognitive abilities, decision-making skills, and problem-solving capabilities. The CRDI is designed to evaluate an individual's ability to process information, make decisions, and solve problems in a structured and systematic manner. The instrument typically consists of a series of questions or scenarios that require the test-taker to make decisions or solve problems based on the available information. The results of the CRDI can be used to identify strengths and weaknesses in cognitive abilities, inform educational and career decisions, and provide insights into an individual's learning style and preferences.\n",
      "\n",
      "*******NEW DATA*******\n",
      "User:192.168.1.7\n",
      "Text Input:\n",
      "Audio input:\n",
      "Output:The image is a black and white photo, possibly a painting or drawing, showcasing a night scene. The main focus is on a large building, which appears to be a church, with a clock tower prominently visible. The clock tower is situated in the center of the building, and the overall scene gives an impression of a historical or architectural significance. The black and white nature of the image adds a sense of timelessness and artistic quality to the composition.\n",
      "\n",
      "*******NEW DATA*******\n",
      "User:192.168.1.7\n",
      "Text Input:What does ATC stands for?\n",
      "Audio input:\n",
      "Output:ATC stands for \"Air Traffic Controller.\" It is a person responsible for managing and directing the movement of aircraft on the ground and in the airspace. They ensure the safe and efficient flow of air traffic, making decisions based on factors such as weather conditions, airspace restrictions, and the positions of other aircraft in the area. ATC plays a crucial role in maintaining the safety and efficiency of air travel.\n",
      "\n",
      "*******NEW DATA*******\n",
      "User:192.168.1.7\n",
      "Text Input:\n",
      "Audio input:\n",
      "Output:The image features a man wearing glasses, with a close-up view of his face. He appears to be a middle-aged individual with a beard. The man is looking directly at the camera, giving the impression of a confident and focused expression. The glasses he is wearing are prominent, and they add a sense of style to his appearance. The overall scene is a close-up of the man's face, emphasizing his features and the glasses he is wearing.\n",
      "\n",
      "*******NEW DATA*******\n",
      "User:192.168.1.7\n",
      "Text Input:What colour is the animal?\n",
      "Audio input:\n",
      "Output:The animal is yellow and brown.\n",
      "Keyboard interruption in main thread... closing server.\n",
      "Killing tunnel 127.0.0.1:7860 <> https://f4d89f1eebcbb251bd.gradio.live\n"
     ]
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#launch with log\n",
    "import torch\n",
    "from transformers import BitsAndBytesConfig, pipeline\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "from PIL import Image\n",
    "import re\n",
    "import whisper\n",
    "import gradio as gr\n",
    "import os\n",
    "from gtts import gTTS\n",
    "import socket\n",
    "\n",
    "def get_ip_address():\n",
    "    hostname = socket.gethostname()\n",
    "    ip_address = socket.gethostbyname(hostname)\n",
    "    return ip_address\n",
    "quantization_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_compute_dtype=torch.float16\n",
    ")\n",
    "\n",
    "model_id = \"llava-hf/llava-1.5-7b-hf\"\n",
    "pipe = pipeline(\"image-to-text\", model=model_id, \n",
    "                model_kwargs={\"quantization_config\": quantization_config})\n",
    "\n",
    "# CUDA device check\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "#print(f\"Using torch {torch._version_} ({DEVICE})\")\n",
    "\n",
    "whisper_model = whisper.load_model(\"base\", device=DEVICE)\n",
    "\n",
    "def transcribe(audio_path):\n",
    "    if not audio_path:\n",
    "        return '', '', None\n",
    "\n",
    "    audio = whisper.load_audio(audio_path)\n",
    "    audio = whisper.pad_or_trim(audio)\n",
    "\n",
    "    mel = whisper.log_mel_spectrogram(audio).to(whisper_model.device)\n",
    "\n",
    "    options = whisper.DecodingOptions(language=\"en\")\n",
    "    result = whisper.decode(whisper_model, mel, options)\n",
    "    return result.text\n",
    "\n",
    "def text_to_speech(text, file_path):\n",
    "    language = 'en'\n",
    "    audio_obj = gTTS(text=text, lang=language, slow=False)\n",
    "    audio_obj.save(file_path)\n",
    "    return file_path\n",
    "\n",
    "def img2txt(input_text, input_image):\n",
    "    image = Image.open(input_image)\n",
    "    prompt_instructions = f\"Act as an expert in imagery descriptive analysis, respond to the prompt: {input_text}\"\n",
    "    prompt = f\"USER: <image>\\n{prompt_instructions}\\nASSISTANT:\"\n",
    "    \n",
    "    outputs = pipe(image, prompt=prompt, generate_kwargs={\"max_new_tokens\": 6000})\n",
    "    \n",
    "    if outputs and len(outputs[0][\"generated_text\"]) > 0:\n",
    "        match = re.search(r'ASSISTANT:\\s*(.*)', outputs[0][\"generated_text\"])\n",
    "        if match:\n",
    "            reply = match.group(1)\n",
    "        else:\n",
    "            reply = \"No response found.\"\n",
    "    else:\n",
    "        reply = \"No response generated.\"\n",
    "    return reply\n",
    "\n",
    "def process_inputs(text_input, audio_path, image_path):\n",
    "    speech_to_text_output = ''\n",
    "    userIP=get_ip_address()\n",
    "    if audio_path:\n",
    "        speech_to_text_output = transcribe(audio_path)\n",
    "    \n",
    "    if text_input:\n",
    "        combined_input = text_input\n",
    "    else:\n",
    "        combined_input = speech_to_text_output\n",
    "\n",
    "    if not combined_input:\n",
    "        combined_input = \"Describe this image.\"\n",
    "\n",
    "    if image_path:\n",
    "        gpt_output = img2txt(combined_input, image_path)\n",
    "    else:\n",
    "        image_path=\"C:/Users/kmano/OneDrive/Documents/Certificates/blk.png\"  # Default image if none provided\n",
    "        gpt_output = img2txt(combined_input, image_path)\n",
    "    \n",
    "    processed_audio_path = text_to_speech(gpt_output, \"Temp3.mp3\") if gpt_output else None\n",
    "    logData=\"\\n*******NEW DATA*******\\nUser:\"+str(userIP)+\"\\nText Input:\"+text_input+\"\\nAudio input:\"+speech_to_text_output+\"\\nOutput:\"+gpt_output\n",
    "    print(logData)\n",
    "    with open('User_Log.txt', 'a') as file:\n",
    "        file.write(logData)\n",
    "    return speech_to_text_output, gpt_output, processed_audio_path\n",
    "\n",
    "iface = gr.Interface(\n",
    "    fn=process_inputs,\n",
    "    inputs=[\n",
    "        gr.Textbox(label=\"Text Input\"),\n",
    "        gr.Audio(type=\"filepath\", label=\"Audio Input\"),  \n",
    "        gr.Image(type=\"filepath\", label=\"Image Input\")   \n",
    "               \n",
    "    ],\n",
    "    outputs=[\n",
    "        gr.Textbox(label=\"Speech to Text Output\"),  \n",
    "        gr.Textbox(label=\"Generated Response\"),     \n",
    "        gr.Audio(label=\"Text to Speech Output\")    \n",
    "    ],\n",
    "    title=\"Intelligent Voice Assistant using Llava\",\n",
    "    description=\"Upload an image, provide voice input, or type a prompt to receive a detailed analysis and audio response.\"\n",
    ")\n",
    "\n",
    "iface.launch(debug=True,share=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4f00b0d-8b12-4d1d-8005-0c24fd8b5066",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
